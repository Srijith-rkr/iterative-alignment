model:
  model_name: 'meta-llama/Llama-2-7b-hf' # replace with "microsoft/phi-2" for debugging DDP 
  cache_dir:  '/data/tir/projects/tir7/user_data/srijithr/hf_cache_dir'
   
lora:
  lora_alpha: 16
  lora_dropout: 0.1
  r: 16
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules: 'k_proj q_proj v_proj'

data:
  train_path: /home/srijithr/iterative-alignment/datasets/alpaca_data_full.json
  output_dir:  /data/tir/projects/tir7/user_data/srijithr/iter_imporv_model_outputs/${sft.run_name}-lr${sft.learning_rate}-bs${sft.per_device_train_batch_size}-ga${sft.gradient_accumulation_steps}-e${sft.num_train_epochs}


sft:
  optim: "paged_adamw_32bit"
  warmup_steps: 100 # warmup_ratio=0.01 is also another option
  max_grad_norm: 1.0
  weight_decay: 0.001

  learning_rate: 1e-4
  lr_scheduler_type: "cosine"

  num_train_epochs: 3 # max_steps=10, # max_steps will override num_train_epoch if it is below num_train_epochs
  per_device_train_batch_size: 2 # Total batch size = per_device_train_batch_size * gradient_accumulation_steps* num_gpus
  gradient_accumulation_steps: 2

  report_to: "wandb"
  run_name: "debugging_SFT_with_Alpaca_data"

  logging_steps: 10
  save_strategy: "epoch"
  evaluation_strategy: "epoch" # eval_steps = 100, # Set this if the evaluation strategy is set to steps
  do_eval: true

  bf16: true  # Adjust based on the GPU (e.g., A100 or 6000Ada)
 # fp16: true # swap above with this if bf16 is not supported (e.g., V100)
  group_by_length: true
  seed: 0




